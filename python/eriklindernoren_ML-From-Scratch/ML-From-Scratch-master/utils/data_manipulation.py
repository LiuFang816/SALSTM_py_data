from __future__ import division
import numpy as np
import math, sys

def shuffle_data(X, y):
	# Concatenate x and y and do a random shuffle
	X_y = np.concatenate((X,y.reshape((1,len(y))).T), axis=1)
	np.random.shuffle(X_y)
	X = X_y[:,:-1] # every column except the last
	y = X_y[:,-1].astype(int) # last column

	return X, y

# Divide dataset based on if sample value on feature index is larger than
# the given threshold
def divide_on_feature(X, feature_i, threshold):
	split_func = None
	if isinstance(threshold, int) or isinstance(threshold, float):
		split_func = lambda sample: sample[feature_i] >= threshold
	else:
		split_func = lambda sample: sample[feature_i] == threshold

	X_1 = np.array([sample for sample in X if split_func(sample)])
	X_2 = np.array([sample for sample in X if not split_func(sample)])

	return np.array([X_1, X_2])

# Return random subsets (with replacements) of the data
def get_random_subsets(X, y, n_subsets, replacements=True):
	n_samples = np.shape(X)[0]
	# Concatenate x and y and do a random shuffle
	X_y = np.concatenate((X,y.reshape((1,len(y))).T), axis=1)
	np.random.shuffle(X_y)
	subsets = []
	
	# Uses 50% of training samples without replacements
	subsample_size = n_samples//2
	if replacements:
		subsample_size = n_samples 		# 100% with replacements

	for _ in range(n_subsets):
		idx = np.random.choice(range(n_samples), size=np.shape(range(subsample_size)), replace=replacements)
		X = X_y[idx][:,:-1]
		y = X_y[idx][:,-1]
		subsets.append([X,y])
	return subsets

# Normalize the dataset X
def normalize(X, axis=-1, order=2):
	l2 = np.atleast_1d(np.linalg.norm(X, order, axis))
	l2[l2==0] = 1
	return X / np.expand_dims(l2, axis)

# Standardize the dataset X
def standardize(X):
	X_std = X
	mean = X.mean(axis=0)
	std = X.std(axis=0)
	for col in range(np.shape(X)[1]):
		if std[col]:
			X_std[:, col] = (X_std[:, col] - mean[col]) / std[col]
	# X_std = (X - X.mean(axis=0)) / X.std(axis=0)
	return X_std

# Split the data into train and test sets
def train_test_split(X, y, test_size=0.5, shuffle=True):
	if shuffle:
		X, y = shuffle_data(X, y)
	# Split the training data from test data in the ratio specified in test_size
	split_i = len(y) - int(len(y)//(1/test_size))
	x_train, x_test = X[:split_i], X[split_i:]
	y_train, y_test = y[:split_i], y[split_i:]

	return x_train, x_test, y_train, y_test

# Split the data into k sets of training / test data
def k_fold_cross_validation_sets(X, y, k, shuffle=True):
	if shuffle:
		X, y = shuffle_data(X, y)

	n_samples = len(y)
	left_overs = {}
	n_left_overs = (n_samples % k)
	if n_left_overs != 0:
		left_overs["X"] = X[-n_left_overs:]
		left_overs["y"] = y[-n_left_overs:]
		X = X[:-n_left_overs]
		y = y[:-n_left_overs]
		
	X_split = np.split(X, k)
	y_split = np.split(y, k)
	sets = []
	for i in range(k):
		X_test, y_test = X_split[i], y_split[i]
		X_train = np.concatenate(X_split[:i] + X_split[i+1:], axis=0)
		y_train = np.concatenate(y_split[:i] + y_split[i+1:], axis=0)
		sets.append([X_train, X_test, y_train, y_test])

	# Add left over samples to last set as training samples
	if n_left_overs != 0:
		np.append(sets[-1][0], left_overs["X"], axis=0)
		np.append(sets[-1][2], left_overs["y"], axis=0)

	return np.array(sets)

# Making an array of nominal values into a binarized matrix
def categorical_to_binary(x):
	n_col = np.amax(x)+1
	binarized = np.zeros((len(x), n_col))
	for i in range(len(x)):
		binarized[i, x[i]] = 1

	return binarized

# Converting from binary vectors to nominal values
def binary_to_categorical(x):
	categorical = []
	for i in range(len(x)):
		if not 1 in x[i]:
			categorical.append(0)
		else:
			i_where_one = np.where(x[i] == 1)[0][0]
			categorical.append(i_where_one)

	return categorical

# Converts a vector into an diagonal matrix
def make_diagonal(x):
	m = np.zeros((len(x), len(x)))
	for i in range(len(m[0])):
		m[i,i] = x[i]

	return m


